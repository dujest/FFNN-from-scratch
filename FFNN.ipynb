{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# the simplest from-scratch approach is to represent vectors as lists of numbers\n",
    "Vector = List[float]\n",
    "\n",
    "def dot(v: Vector, w: Vector) -> float:\n",
    "\n",
    "    assert len(v) == len(w) # vectors mu be same length\n",
    "\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron computes a weighted sum of its inputs and “fires” if that weighted sum is 0 or greater:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x: float) -> float:\n",
    "    return 1.0 if x >= 0 else 0.0\n",
    "\n",
    "\n",
    "def perceptron_output(weights: Vector, bias: float, x: Vector) -> float:\n",
    "    \"\"\"Returns 1 if the perceptron 'fires', 0 if not\"\"\"\n",
    "    calculation = dot(weights, x) + bias\n",
    "    return step_function(calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-Forward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train a neural network, we need to use calculus, and in order to use calculus, we need smooth functions.\n",
    "`step_function` isn’t even continuous, and sigmoid is a good smooth approximation of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid (t: float) -> float:\n",
    "    return 1 / (1 + math.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid2 (t: float) -> float:\n",
    "    return 1 / (1 + np.exp(-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent a neural network as a list of layers, where each layer is just a list of the neurons (vectors of weights) in that layer.\n",
    "\n",
    "Neural Network = list (layers) of lists (neurons) of vectors (weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_output(weights: Vector, inputs: Vector) -> float:\n",
    "        return sigmoid(dot(weights, inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(neural_network: List[List[Vector]],\n",
    "                 input_vector: Vector) -> List[Vector]:\n",
    "    \"\"\"\n",
    "    Feeds the input vector through the neural network.\n",
    "    Returns the outputs of all layers (not just the last one).\n",
    "    \"\"\"\n",
    "    outputs: List[Vector] = []\n",
    "\n",
    "    for layer in neural_network:\n",
    "        input_with_bias = input_vector + [1]              # Add a constant.\n",
    "        output = [neuron_output(neuron, input_with_bias)  # Compute the output\n",
    "                  for neuron in layer]                    # for each neuron.\n",
    "        outputs.append(output)                            # Add to results.\n",
    "\n",
    "        # Then the input to the next layer is the output of this one\n",
    "        input_vector = output\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_network = [# hidden layer\n",
    "               [[20., 20, -30],      # 'and' neuron\n",
    "                [20., 20, -10]],     # 'or'  neuron\n",
    "               # output layer\n",
    "               [[-60., 60, -30]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward(xor_network, [0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed_forward returns the outputs of all layers, so the [-1] gets the\n",
    "# final output, and the [0] gets the value out of the resulting vector\n",
    "feed_forward(xor_network, [0, 0])[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward(xor_network, [0, 1])[-1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use data to train neural networks. The typical approach is an algorithm called backpropagation, \n",
    "which uses the gradient descent algorithm.\n",
    "\n",
    "Imagine we have a training set that consists of input vectors and corresponding target\n",
    "output vectors. For example, in our previous xor_network example, the input vector\n",
    "[1, 0] corresponded to the target output [1]. Imagine that our network has some set\n",
    "of weights. We then adjust the weights using the following algorithm:\n",
    "\n",
    "1. Run feed_forward on an input vector to produce the outputs of all the neurons\n",
    "in the network.\n",
    "2. We know the target output, so we can compute a loss that’s the sum of the\n",
    "squared errors.\n",
    "3. Compute the gradient of this loss as a function of the output neuron’s weights.\n",
    "4. “Propagate” the gradients and errors backward to compute the gradients with\n",
    "respect to the hidden neurons’ weights.\n",
    "5. Take a gradient descent step or update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(network: List[List[Vector]],\n",
    "              input_vector: Vector,\n",
    "              target_vector: Vector) -> List[List[Vector]]:\n",
    "    \"\"\"\n",
    "    Given a neural network, an input vector, and a target vector,\n",
    "    make a prediction and compute the gradient of the squared error\n",
    "    loss with respect to the neuron weights.\n",
    "    \"\"\"\n",
    "    # forward pass\n",
    "    hidden_outputs, outputs = feed_forward(network, input_vector)\n",
    "\n",
    "    # gradients with respect to output neuron pre-activation outputs\n",
    "    # the derivative of a sigmoid function is output * (1 - output)\n",
    "    output_deltas = [output * (1 - output) * (output - target)\n",
    "                     for output, target in zip(outputs, target_vector)]\n",
    "\n",
    "    # gradients with respect to output neuron weights\n",
    "    output_grads = [[output_deltas[i] * hidden_output\n",
    "                     for hidden_output in hidden_outputs + [1]]\n",
    "                    for i, output_neuron in enumerate(network[-1])]\n",
    "\n",
    "    # gradients with respect to hidden neuron pre-activation outputs\n",
    "    # the derivative of a sigmoid function is output * (1 - output)\n",
    "    hidden_deltas = [hidden_output * (1 - hidden_output) *\n",
    "                         dot(output_deltas, [n[i] for n in network[-1]])\n",
    "                     for i, hidden_output in enumerate(hidden_outputs)]\n",
    "\n",
    "    # gradients with respect to hidden neuron weights\n",
    "    hidden_grads = [[hidden_deltas[i] * input for input in input_vector + [1]]\n",
    "                    for i, hidden_neuron in enumerate(network[0])]\n",
    "\n",
    "    return [hidden_grads, output_grads]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically we run this algorithm many times for our entire training set until the network\n",
    "converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(v: Vector) -> float:\n",
    "    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "\n",
    "def magnitude(v: Vector) -> float:\n",
    "    \"\"\"Returns the magnitude (or length) of v\"\"\"\n",
    "    return math.sqrt(sum_of_squares(v))   # math.sqrt is square root function\n",
    "\n",
    "\n",
    "def subtract(v: Vector, w: Vector) -> Vector:\n",
    "    \"\"\"Subtracts corresponding elements\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be the same length\"\n",
    "\n",
    "    return [v_i - w_i for v_i, w_i in zip(v, w)]\n",
    "\n",
    "\n",
    "def distance(v: Vector, w: Vector) -> float:\n",
    "    return magnitude(subtract(v, w))\n",
    "\n",
    "\n",
    "def scalar_multiply(c: float, v: Vector) -> Vector:\n",
    "    \"\"\"Multiplies every element by c\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "\n",
    "def add(v: Vector, w: Vector) -> Vector:\n",
    "    \"\"\"Adds corresponding elements\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be the same length\"\n",
    "\n",
    "    return [v_i + w_i for v_i, w_i in zip(v, w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    import random\n",
    "    random.seed(0)\n",
    "    \n",
    "    # training data\n",
    "    xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
    "    ys = [[0.], [1.], [1.], [0.]]\n",
    "    \n",
    "    # start with random weights\n",
    "    network = [ # hidden layer: 2 inputs -> 2 outputs\n",
    "                [[random.random() for _ in range(2 + 1)],   # 1st hidden neuron\n",
    "                 [random.random() for _ in range(2 + 1)]],  # 2nd hidden neuron\n",
    "                # output layer: 2 inputs -> 1 output\n",
    "                [[random.random() for _ in range(2 + 1)]]   # 1st output neuron\n",
    "              ]\n",
    "        \n",
    "    learning_rate = 1.0\n",
    "    \n",
    "    for epoch in tqdm.trange(20000, desc=\"neural net for xor\"):\n",
    "        for x, y in zip(xs, ys):\n",
    "            gradients = gradients(network, x, y)\n",
    "    \n",
    "            # Take a gradient step for each neuron in each layer\n",
    "            network = [[gradient_step(neuron, grad, -learning_rate)\n",
    "                        for neuron, grad in zip(layer, layer_grad)]\n",
    "                       for layer, layer_grad in zip(network, gradients)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ml-yrp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4be6c2bb49d24515f1650ecf25b3e93e0cda5b5fbe8c944f724117236b6fd181"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
